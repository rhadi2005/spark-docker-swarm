version: '3.8'

# command to build the container image & upload to docker hub
#docker build -t rhadi2005/spark-swarm:3.1.2 .
#docker login -u rhadi2005
#docker push rhadi2005/spark-swarm:3.1.2

services:

  master:
    #image: rhadi2005/spark-swarm:3.1.2
    image: circlebi/forecast
    #command: cd ${SPARK_HOME} && bin/spark-class org.apache.spark.deploy.master.Master -h 0.0.0.0
    #command: bash 
    hostname: master
    environment:
      - SPARK_DRIVER_MEMORY=16G
      - SPARK_DRIVER_MAXRESULTSIZE=16G
      - MASTER=spark://0.0.0.0:7077
      - SPARK_CONF_DIR=/conf
      - SPARK_PUBLIC_DNS=${EXTERNAL_IP}
      - SPARK_MASTER_HOST=0.0.0.0
      - SPARK_MASTER_PORT=7077
    extra_hosts:
      - ecs-python2:10.50.0.194
    ports:
#      - 4040:4040
#      - 6066:6066
      - 7077:7077
      - 8088:8080
    # networks:
    #   spark_net:
    #     ipv4_address: 172.16.238.10
    #     ipv6_address: 2001:3984:3989::10
    volumes:
      - /home/cloud/dataset:/home/cloud/dataset
    deploy:
        placement:
            constraints:
              - node.role==manager

  small-worker:
    #image: rhadi2005/spark-swarm:3.1.2
    image: circlebi/forecast
    command: cd ${SPARK_HOME} && bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    hostname: small-worker
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
      # - SPARK_EXECUTOR_MEMORY=16G # this should be specified by the application
      - SPARK_CONF_DIR=/conf
      - SPARK_PUBLIC_DNS=${EXTERNAL_IP}
      - SPARK_MASTER_HOST=master
      - SPARK_MASTER_PORT=7077
    extra_hosts:
      - ecs-python2:10.50.0.194
    ports:
      - 8082:8081
    deploy:
      replicas: 0 
    depends_on:
      - master
    volumes:
      - /home/cloud/dataset:/home/cloud/dataset
    deploy:
        placement:
            constraints:
              - node.role==manager

  worker:
    #image: rhadi2005/spark-swarm:3.1.2
    image: circlebi/forecast
    command: cd ${SPARK_HOME} && bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    hostname: worker
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=16G
      # - SPARK_WORKER_MEMORY=512m
      # - SPARK_EXECUTOR_MEMORY=16G # this should be specified by the application
      - SPARK_CONF_DIR=/conf
      - SPARK_PUBLIC_DNS=${EXTERNAL_IP}
      - SPARK_MASTER_HOST=master
      - SPARK_MASTER_PORT=7077
    extra_hosts:
      - ecs-python2:10.50.0.194
    ports:
      - 8081:8081
    depends_on:
      - master
    volumes:
      - /home/cloud/dataset:/home/cloud/dataset
    deploy:
      replicas: 0
      placement:
          constraints:
            - node.role==worker
            #- node.role==manager

# networks:
#   spark_ingress:
#     ingress:
#   spark_net:
#     ipam:
#       driver: default
#       config:
#         - subnet: "172.16.238.0/24"
#         - subnet: "2001:3984:3989::/64"

